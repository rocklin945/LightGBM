# MR 风险评估项目 - 重构完成

## 🎯 项目概述

本项目实现了一个全自动的 MR（Merge Request）风险评估系统，通过机器学习模型判断 MR 是否可以自动合并，无需人工介入。

## ✨ 核心特性

### 1. 全自动标注
- **规则标注**：基于明确规则自动判断
- **评分标注**：对不确定样本进行评分（60分及以上可合并）
- **无需人工**：100%自动化，从数据生成到标注完全自动

### 2. 多模型支持
- LightGBM
- Logistic Regression
- SVM
- KNN
- Random Forest
- MLP (多层感知机)

### 3. 完整的工作流
```
数据生成 → 自动标注 → 批量训练 → 模型对比
```

## 📁 目录结构

```
LightGBM/
├── data/                      # 数据目录
│   ├── raw_mr_data.csv       # 原始数据
│   ├── labeled_data.csv      # 已标注数据
│   └── test_sets/            # 测试集
│
├── utils/                     # 工具脚本
│   ├── generate_data.py      # 数据生成（可控制数量）
│   ├── auto_label.py         # 全自动标注
│   └── create_test_set.py    # 测试集生成
│
├── models/                    # 模型目录
│   ├── lightgbm/             # LightGBM 模型
│   ├── logistic/             # 逻辑回归
│   ├── svm/                  # SVM
│   ├── knn/                  # KNN
│   ├── random_forest/        # 随机森林
│   ├── mlp/                  # 多层感知机
│   └── comparison/           # 模型对比
│       ├── test_all_models.py
│       └── results/
│
├── config.py                 # 配置文件
├── train_all_models.py       # 批量训练
└── README.md
```

## 🚀 快速开始

### 1. 生成数据
```bash
python utils/generate_data.py
```
- 默认生成 500 条样本
- 50 个核心特征
- 可自定义数量

### 2. 全自动标注
```bash
python utils/auto_label.py
```
- 基于规则和评分系统
- 100% 自动化标注
- 无需人工介入

### 3. 批量训练
```bash
python train_all_models.py
```
- 自动训练所有 6 个模型
- 生成训练结果图表
- 保存模型文件

### 4. 模型对比
```bash
python models/comparison/test_all_models.py
```
- 生成测试数据
- 对比所有模型性能
- 生成对比图表

## 📊 特征说明

项目使用 **50 个核心特征**，分为 10 个类别：

1. **代码变更规模**（5个）：总改动行数、新增/删除行数等
2. **文件类型**（5个）：代码/测试文件数、改动占比等
3. **代码复杂度**（4个）：方法数、圈复杂度、嵌套层级等
4. **关键变更**（4个）：依赖、数据库、API、破坏性变更
5. **测试质量**（5个）：单元测试、覆盖率等
6. **CI/CD 质量**（6个）：构建、测试、规范检查等
7. **作者质量**（9个）：经验、合并率、回滚率等
8. **提交质量**（3个）：提交次数、大小、信息质量
9. **仓库权限**（3个）：重要性、Owner、Maintainer
10. **风险信号**（6个）：支付、PII、权限、安全等

## 🔧 自动标注规则

### 明确可以合并
- 纯文档改动
- 纯测试改动且通过
- Owner/Maintainer 小改动
- 核心贡献者微小改动
- 资深作者 + 小改动 + 高质量

### 明确不可以合并
- 涉及支付或 PII
- CI 构建/测试失败
- 破坏性变更
- 数据库迁移
- 存在安全漏洞
- 改动过大（>500行）
- 作者经验不足
- 代码质量问题严重

### 评分标注（不确定情况）
- 作者质量：20 分
- 代码规模：15 分
- CI 质量：15 分
- 测试质量：10 分
- 代码质量：10 分
- 风险扣分：最多 -20 分

**阈值：60 分及以上可合并**

## 📈 评估指标

- **准确率 (Accuracy)**：整体预测正确率
- **精确率 (Precision)**：预测可合并中实际可合并的比例
- **召回率 (Recall)**：实际可合并中被正确预测的比例
- **F1 分数**：精确率和召回率的调和平均
- **AUC**：ROC 曲线下面积

## 🎨 可视化图表

### 单个模型训练结果
- 左上：特征重要性 Top 15（或模型说明）
- 右上：预测概率分布
- 左下：混淆矩阵
- 右下：评估指标

### 模型对比图表
- 准确率/AUC/F1 对比
- ROC 曲线对比
- 精确率/召回率对比
- 前 3 个模型的混淆矩阵

## ⚙️ 配置说明

在 `config.py` 中可以修改：
- 特征列表
- 模型参数
- 自动合并阈值（默认 0.6）
- 强制拒绝条件

## 📝 注意事项

1. **数据量**：建议 500 条以上
2. **样本平衡**：正负样本比例 40-60%
3. **特征数量**：50 个特征已优化，不建议随意增减
4. **阈值调整**：根据实际需求调整 AUTO_MERGE_THRESHOLD
5. **图表存储**：所有图表自动保存，不弹窗

## 🔄 完整工作流

```
1. python utils/generate_data.py        # 生成 500 条数据
2. python utils/auto_label.py           # 全自动标注
3. python train_all_models.py           # 批量训练 6 个模型
4. python utils/create_test_set.py      # 生成测试集
5. python models/comparison/test_all_models.py  # 模型对比
```

## 📊 重构改进

### 之前的问题
- 目录结构混乱
- 需要人工标注（费时费力）
- 图表弹窗影响批量处理
- 文件分散不易管理

### 重构后的优势
- ✅ 清晰的目录结构
- ✅ 100% 全自动标注
- ✅ 所有图表文件存储
- ✅ 独立的模型文件夹
- ✅ 统一的工具脚本
- ✅ 完善的文档说明

## 🎯 使用场景

- 自动化 MR 评审流程
- 降低人工评审成本
- 提高合并效率
- 保证代码质量和安全性

## 📄 许可

MIT License
